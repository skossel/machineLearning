{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyN2fBThgo8wJQn6Xf6V6crC"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.132413Z",
     "start_time": "2025-09-11T14:46:27.126421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from django.contrib.admin.templatetags.admin_list import results\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.155862Z",
     "start_time": "2025-09-11T14:46:27.148460Z"
    }
   },
   "cell_type": "code",
   "source": "tf.__version__",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup Tensorboard"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.193503Z",
     "start_time": "2025-09-11T14:46:27.187404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "file_name = 'my_saved_model'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\\\{}\".format(file_name))"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.391997Z",
     "start_time": "2025-09-11T14:46:27.214516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# augments the image with zoom, mirroring, feature scaling etc.\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rescale: applies feature scaling for each pixel. A pixel is a number between 0-255. When you multiply this nummber for example 128 * rescale of 1/255 you get 0.5. The output number is normalized between 0 and 1.\n",
    "    rescale = 1./255,\n",
    "    # Bild wird verzogen. Auf wenn das Bild verzogen ist, soll das Objekt noch erkennt werden auf dem Bild. Dies ist ein Schritt für die Datenerweiterung (Augmentation). The value 0.2 means you can incline the image until 11 degree.\n",
    "    shear_range = 0.2,\n",
    "    # Zoom range of 0.2: it's a part of the data augmentation and will zoom in for max 20% and can zoom out for max 20%. This will help to improve the quality of the image recognition because with that the model can also recognize a object in the image when it's bigger oder smaller. E.g. the cat is in the background of the image (zoom-out) or on the image is only the head of the cat without the body (zoom-in).\n",
    "    zoom_range = 0.2,\n",
    "    # horizontal_flip will randomly mirror the images. E.g. on the image is a cat which look to the right. This parameter will mirror this image and afterwards the cat will look to the left. To set this parameter to true will help the model to learn that the direction if the cat looks to the left or to the right doesn't matter. The label will stay a cat.\n",
    "    horizontal_flip = True\n",
    ")\n",
    "\n",
    "# connects the dataframe with the path of the training images\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    './dataset/training_set',\n",
    "    # all images will be resized from their original size to a image of 64px width and 64px height\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.467605Z",
     "start_time": "2025-09-11T14:46:27.408789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    './dataset/test_set',\n",
    "    target_size = (64, 64),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'binary'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.495781Z",
     "start_time": "2025-09-11T14:46:27.490343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sequential: seqzence of layers\n",
    "cnn = tf.keras.models.Sequential()"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.529988Z",
     "start_time": "2025-09-11T14:46:27.511882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# filters = 32: How many features should be detected. 32 means the model will detect 32 different features in the image (e.g. nose, mouth)\n",
    "# kernel_size = 3: kernel means feature map and is in this example a 3x3 Pixels Block which is \"pushed\" over the image to detect where is a feature in the image\n",
    "# input_shape = [64, 64, 3]: 64px widht, 64px height and 3 colors rgb. For Black-white image you whould enter 1.\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64, 64, 3]))"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.555445Z",
     "start_time": "2025-09-11T14:46:27.547503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pool_size = 2: a 2x2 Pixels Block will be used for max pooling\n",
    "# strides = 2: How many Pixels the block should change. For strides of 2 the change will be 2 Pixels. E.g. for a 2x2 block the next the will be after 2 Pixels therefore it will take the next 2x2 block an not the result of the old one.\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.604311Z",
     "start_time": "2025-09-11T14:46:27.587459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Note: the input_shape parameter is only required in the first hidden layer to connect with the input layer. In this Layer, the second hidden layer, this parameter isn't needed.\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
    "# Note: pool_size and strides should be the same number to have a smooth transition\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.635325Z",
     "start_time": "2025-09-11T14:46:27.621367Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Flatten())",
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.680500Z",
     "start_time": "2025-09-11T14:46:27.658848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this layer takes all the features of the vector from the flattening step und connects this features to the neurons in the full connection layer.\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.706139Z",
     "start_time": "2025-09-11T14:46:27.691572Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:46:27.727191Z",
     "start_time": "2025-09-11T14:46:27.718199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.compile(\n",
    "    # weight adjustement with gradient descent (adam -> Adaptive Moment Estimation)\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:49:16.651704Z",
     "start_time": "2025-09-11T14:46:27.744794Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.fit(x = training_set, validation_data = test_set, epochs = 25, callbacks = [tensorboard])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 111ms/step - accuracy: 0.5913 - loss: 0.6630 - val_accuracy: 0.6990 - val_loss: 0.5873\n",
      "Epoch 2/5\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 148ms/step - accuracy: 0.6951 - loss: 0.5800 - val_accuracy: 0.6765 - val_loss: 0.5904\n",
      "Epoch 3/5\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 144ms/step - accuracy: 0.7210 - loss: 0.5416 - val_accuracy: 0.7240 - val_loss: 0.5381\n",
      "Epoch 4/5\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 138ms/step - accuracy: 0.7404 - loss: 0.5174 - val_accuracy: 0.7495 - val_loss: 0.5085\n",
      "Epoch 5/5\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 131ms/step - accuracy: 0.7527 - loss: 0.5001 - val_accuracy: 0.7615 - val_loss: 0.4981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x188d710fec0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:49:20.915057Z",
     "start_time": "2025-09-11T14:49:16.858964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# loads the tensorboard extension\n",
    "%load_ext tensorboard\n",
    "# shows the path to the log file\n",
    "%tensorboard --logdir logs/my_saved_model/train"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ddb092b23c1fc58f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ddb092b23c1fc58f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:49:21.275868Z",
     "start_time": "2025-09-11T14:49:20.943455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "# add extra fake dimension for the batch_size.\n",
    "# axis = 0: this dimension will be on the first axis\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "# result[0][0]: first [0] is for the number of batch -> the first batch. Second [0] is for the number of element in the batch -> the first und single element\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 173ms/step\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T14:49:21.329737Z",
     "start_time": "2025-09-11T14:49:21.322942Z"
    }
   },
   "cell_type": "code",
   "source": "print(prediction)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ]
}
